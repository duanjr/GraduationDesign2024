## littlefs 的设计

littlefs 是为微控制器设计的一个小型容错文件系统。面临的问题是：如何构建一个在断电和闪存磨损情况下具有弹性的文件系统，而不使用无限制的内存？

本文档涵盖了 littlefs 的高层设计，它与其他文件系统的不同之处，以及导致我们设计到目前为止的决策。

## 问题

littlefs 针对的嵌入式系统通常是具有约 32 KiB RAM 和 512 KiB ROM 的 32 位微控制器。这些通常与具有约 4 MiB 闪存存储的 SPI NOR 闪存芯片配对。这些设备对于 Linux 和大多数现有文件系统来说太小了，需要专门针对大小编写的代码。

闪存本身是一种有其自身特性和细微差别的有趣技术。与其他存储形式不同，写入闪存需要两步操作：擦除和编程。编程（将位设置为 0）相对便宜，并且可以非常细粒度。然而擦除（将位设置为 1）需要昂贵且具有破坏性的操作，这也是闪存得名的原因。

更令人烦恼的是，这些嵌入式系统很容易在任何时候失去电源。通常，微控制器代码简单且反应式，没有关机例程的概念。这对持久存储提出了巨大挑战，不幸的断电可能会损坏存储并使设备无法恢复。

这使得我们对于嵌入式文件系统有三个主要要求。

1. **抗断电性** - 在这些系统中，电源可能随时丢失。如果断电导致任何持久数据结构损坏，可能会导致设备无法恢复。嵌入式文件系统必须设计为能够在任何写操作期间从断电中恢复。
2. **磨损均衡** - 写入闪存是破坏性的。如果文件系统反复写入同一块，最终该块会磨损。未考虑磨损的文件系统可能很容易烧坏用于存储频繁更新的元数据的块，导致设备提前失效。
3. **有限的 RAM/ROM** - 如果以上要求还不够，这些系统的内存量也非常有限。这阻止了许多现有的文件系统设计，因为它们可能依赖相对较大的 RAM 来临时存储文件系统元数据。

对于 ROM，这意味着我们需要保持设计简单，并尽可能重用代码路径。对于 RAM，我们有更严格的要求，所有 RAM 使用都是有界的。这意味着 RAM 使用不会随着文件系统的大小或文件数量的变化而增长。这带来了一个独特的挑战，即使是看似简单的操作，如遍历文件系统，也变得异常困难。

## 现有设计

那么，已经存在的有哪些设计呢？当然，有许多不同的文件系统，但它们通常共享和借鉴彼此的特性。如果我们关注抗断电性和磨损均衡，可以将其缩小到少数几种设计。

1. 首先是非抗断电的基于块的文件系统，如 FAT 和 ext2。这些是最早的文件系统设计，通常也是最简单的。在这里，存储被划分为块，每个文件存储在一组块中。未经修改，这些文件系统不具备抗断电性，因此更新文件仅需就地重写块。

   因为它们的简单性，这些文件系统通常是最快且最小的。然而缺乏抗断电性不是很好，存储位置和数据的绑定关系使得文件系统无法管理磨损。

2. 在完全不同的方向上，有日志文件系统，如 JFFS、YAFFS 和 SPIFFS，存储位置不绑定到数据块，而是整个存储被用作循环日志，记录每次对文件系统的更改。写入时附加新更改，而读取则需要遍历日志以重建文件。一些日志文件系统缓存文件以避免读取成本，但这会带来 RAM 的权衡。

   日志文件系统非常优雅。通过校验和，我们可以轻松检测断电并通过忽略失败的附加操作回退到以前的状态。如果这还不够好，它们的循环特性意味着日志文件系统可以完美地分布磨损在存储中。

   主要缺点是性能。如果我们看看垃圾收集，即从日志末尾清理过时数据的过程，我尚未看到一个纯日志文件系统不具备以下两个成本之一：

   1. *O(n²)* 运行时
   2. *O(n)* RAM

   SPIFFS 是一个非常有趣的案例，因为它利用了对 NOR 闪存的重复编程既是原子的也是掩码的。这是一个非常巧妙的解决方案，但它限制了你可以支持的存储类型。

3. 也许是最常见类型的文件系统，日志文件系统是将基于块的文件系统与日志文件系统结合而成的产物。ext4 和 NTFS 是很好的例子。在这里，我们采用一个正常的基于块的文件系统，并添加一个有界日志，在更改发生前记录每个更改。

   这种文件系统结合了两者的优势。性能可以和基于块的文件系统一样快（尽管更新日志有小的成本），而对日志的原子更新使文件系统能够在断电事件中恢复。

   不幸的是，日志文件系统有几个问题。它们相当复杂，因为实际上有两个文件系统并行运行，这带来了代码大小的成本。它们也不提供磨损保护，因为存储位置和数据之间的强绑定关系。

4. 最后但同样重要的是，复制写入（COW）文件系统，如 btrfs 和 ZFS。它们与其他基于块的文件系统非常相似，但不是就地更新块，而是通过创建包含更改的副本并用新块替换旧块的引用来执行所有更新。这会递归地将所有问题向上推直到文件系统的根，通常存储在一个非常小的日志中。

   COW 文件系统很有趣。它们提供了与基于块的文件系统非常相似的性能，同时成功地实现了无需将数据更改直接存储在日志中的原子更新。它们甚至取消了数据存储位置的关联，为磨损均衡创造了机会。

   不过，几乎是这样。更新的无限向上移动引发了一些问题。因为对 COW 文件系统的更新不会停止，直到它们到达根，这意味着一个更新可能会导致比原始数据所需更多的写入。此外，向上的运动将这些写入集中到一个块中，这可能比文件系统的其余部分更早磨损。

## littlefs

那么 littlefs 做了什么？

如果我们看看现有的文件系统，有两种有趣的设计模式脱颖而出，但各有一套问题。日志记录提供了独立的原子性，但运行时性能差。COW 数据结构性能良好，但将原子性问题向上推。

我们能绕过这些限制吗？

考虑日志记录。它要么有 *O(n²)* 运行时，要么有 *O(n)* RAM 成本。我们无法避免这些成本，但如果我们对大小设定上限，至少可以防止理论成本成为问题。这依赖于一个超级秘密的计算机科学技巧，即通过限制输入，可以假装任何算法复杂性都是 *O(1)*。

在 COW 数据结构的情况下，我们可以尝试稍微改变定义。假设我们的 COW 结构不是在每次写入后复制，而是在 *n* 次写入后复制。这不会改变大多数 COW 属性（假设你可以原子地写入！），但它确实防止了磨损的向上运动。这种有界写入复制（CObW）仍然集中磨损，但在每一层，我们通过 *n* 除以磨损的传播。对于足够大的 *n*（大于分支因子），磨损传播不再是问题。

明白了吗？分离的日志记录和 COW 是不完美的解决方案，具有限制其有用性的弱点。但如果我们将两者合并，它们可以相互解决彼此的限制。

这就是 littlefs 背后的理念。在子块级别，littlefs 由小型的双块日志构建，提供文件系统任何地方元数据的原子更新。在超块级别，littlefs 是一个 CObW 块树，可以按需驱逐。

还有一些小问题。小日志在存储方面可能昂贵，最坏情况下，一个小日志的成本是原始数据的 4 倍。CObW 结构需要高效的块分配器，因为每 *n* 次写入都需要分配。并且仍然存在保持 RAM 使用恒定的挑战。

## 元数据对

元数据对是 littlefs 的核心。这些是小型的双块日志，允许文件系统任何地方的元数据原子更新。

为什么需要两个块？因为日志通过在磁盘上存储的循环缓冲区附加条目来工作。但记住闪存的写入粒度有限。我们可以在擦除的块上增量编程新数据，但需要一次擦除一个完整块。这意味着为了让我们的循环缓冲区工作，我们需要不止一个块。

我们可以使日志大于两个块，但下一个挑战是如何存储这些日志的引用。因为块本身在写入期间被擦除，使用数据结构来跟踪这些块是复杂的。简单的解决方案是在每个元数据对中存储两个块地址。这还有一个附加优点，即我们可以独立地更换元数据对中的块，并且不会减少其他操作的块粒度。

为了确定哪个元数据块是最新的，我们存储一个修订计数，并使用 [序列算术] 进行比较（非常有用以避免整数溢出的问题）。巧妙的是，这个修订计数也让我们大致了解块上发生了多少次擦除。

因此，如何原子地更新我们的元数据对？原子性（断电抗性的一种）需要两部分：冗余和错误检测。错误检测可以通过校验和提供，在 littlefs 的情况下，我们使用 32 位 [CRC]。另一方面，保持冗余需要多个阶段。

1. 如果我们的块未满，且编程大小足够小以允许我们附加更多条目，我们可以简单地将条目附加到日志中。因为我们不覆盖原始条目（记住重写闪存需要擦除），如果在附加过程中断电，我们仍然有原始条目。

   注意，littlefs 并不为每个条目维护校验和。许多日志文件系统这样做，但它限制了你可以在单个原子操作中更新的内容。相反，我们可以将多个条目分组到一个共享单个校验和的提交中。这让我们能够更新多个不相关的元数据，只要它们驻留在同一个元数据对中。

2. 如果我们的块 *已满*，我们需要以某种方式移除过时的条目以为新条目腾出空间。这个过程称为垃圾收集，但因为 littlefs 有多个垃圾收集器，我们也称这个特定情况为压缩。

   与其他文件系统相比，littlefs 的垃圾收集器相对简单。我们希望避免 RAM 消耗，因此使用一种蛮力解决方案，对于每个条目，我们检查是否有较新的条目已被写入。如果条目是最新的，我们将其附加到新块。这就是拥有两个块变得重要的原因，如果断电，我们仍然在原始块中保留所有内容。

   在这个压缩步骤中，我们还擦除元数据块并增加修订计数。因为我们可以一次提交多个条目，我们可以将所有这些更改写入第二个块，而无需担心断电。只有当提交的校验和被写入时，压缩后的条目和修订计数才被提交和可读。

3. 如果我们的块已满且我们无法找到任何垃圾，那么怎么办？此时，大多数日志文件系统会返回一个指示没有更多空间可用的错误，但因为我们有小日志，日志溢出实际上并不是一个错误条件。

   相反，我们将原始元数据对分成两个元数据对，每个元数据对包含一半的条目，通过尾指针连接。我们不是增加日志的大小并处理与较大日志相关的可扩展性问题，而是形成一个小有界日志的链表。这是一种权衡，因为这种方法确实使用了更多的存储空间，但有利于提高可扩展性。

尽管写入了两个元数据对，我们仍然可以在拆分步骤中保持抗断电性，方法是先准备新的元数据对，然后在对原始元数据对提交时插入尾指针。

另外一个在处理小日志时出现的复杂性是，垃圾收集的摊销运行时成本不仅取决于其一次性成本（littlefs 的 *O(n²)*），还取决于垃圾收集发生的频率。

考虑两个极端：

1. 日志为空，垃圾收集每 *n* 次更新发生一次
2. 日志已满，垃圾收集**每次**更新都发生

显然，我们需要比等待元数据对满时更积极。当元数据对接近满时，压缩的频率迅速增加。

泛泛地看问题，考虑一个每个条目有 ![n] 字节的日志，![d] 动态条目（在垃圾收集中被过时的条目），以及 ![s] 静态条目（在垃圾收集中需要复制的条目）。如果我们看更新这个日志的摊销运行时复杂性，我们得到这个公式：

cost = n + n (s / d+1)

如果我们让 r 是静态空间与日志大小（字节）的比率，我们找到静态和动态条目的另一种表示：

s = r (size/n)

d = (1 - r) (size/n)

将它们代入 d 和 s，得到一个关于日志满度的更新条目的成本的公式：

cost = n + n (r (size/n) / ((1-r) (size/n) + 1))

假设每个条目 100 字节，在一个 4 KiB 的日志中，我们可以使用条目大小来绘制一个乘法成本图：

在 50% 使用率时，我们看到每次更新的平均成本是 2 倍，而在 75% 使用率时，平均成本已经是 4 倍。

为了避免这种指数增长，我们不是等待元数据对满了之后再拆分，而是在超过 50% 容量时拆分。我们通过懒惰地等待需要压缩后再检查是否符合 50% 限制来实现。这将垃圾收集的开销限制在运行时成本的 2 倍，给我们一个摊销运行时复杂性为 *O(1)*。

------

如果我们从高层次看元数据对和元数据对的链表，它们具有相当不错的运行时成本。假设有 *n* 个元数据对，每个元数据对包含 *m* 个元数据条目，特定条目的 *查找* 成本的最坏情况运行时复杂性是 *O(nm)*。对于 *更新* 特定条目，最坏情况复杂性是 *O(nm²)*，摊销复杂性仅为 *O(nm)*。

然而，在 50% 容量时拆分意味着在最佳情况下，我们的元数据对只有一半满。如果我们包括元数据对中第二个块的开销，每个元数据条目的有效存储成本是原始大小的 4 倍。我想用户不会高兴地发现他们只能使用原始存储的四分之一。元数据对提供了一种执行原子更新的机制，但我们需要一个单独的机制来存储数据的大部分。

## CTZ 跳表

元数据对提供了高效的原子更新，但不幸的是有很高的存储成本。但我们可以通过仅使用元数据对存储引用到更密集的复制写入（COW）数据结构来规避这种存储成本。

复制写入数据结构，也称为纯函数数据结构，是一类数据结构，其基础元素是不可变的。对数据进行更改需要创建包含更新数据的新的元素副本，并用新元素的引用替换任何引用。通常，COW 数据结构的性能取决于在替换数据部分后可以重用多少旧元素。

littlefs 对其 COW 结构有几个要求。它们需要高效地读取和写入，但最令人沮丧的是，它们需要以恒定的 RAM 遍历。值得注意的是，这排除了 B 树，因为它们无法以恒定 RAM 遍历，以及 B+-树，因为它们无法通过 COW 操作更新。

------

那么，我们能做什么？首先，让我们考虑将文件存储在一个简单的 COW 链表中。附加一个块，这是写入文件的基础，意味着我们必须更新最后一个块以指向新的块。这需要一个 COW 操作，这意味着我们需要更新倒数第二个块，然后是倒数第三个，依此类推，直到我们复制整个文件。

为了避免在附加时进行完整复制，我们可以反向存储数据。附加块只需要添加新块，不需要更新其他块。如果我们在中间更新一个块，我们仍然需要复制后续块，但可以重用任何之前的块。由于大多数文件写入是线性的，这种设计赌定了附加是最常见的数据更新类型。

然而，反向链表有一个明显的问题。按顺序遍历文件的运行时成本是 *O(n²)*。仅仅为了读取一个文件就有二次运行时成本！这太糟糕了。

幸运的是，我们可以做得更好。littlefs 使用一个多层链表，通常称为跳表。然而，与最常见类型的跳表不同，littlefs 的跳表是严格确定性的，基于计数尾随零（CTZ）指令的一些有趣属性。

CTZ 跳表遵循的规则是，每个可以被 2 的 *k* 次方整除的 *n* 第 *n* 块包含一个指向块 *n - 2^k* 的指针。这意味着每个块包含 1 到 log₂_n_ 个指针，跳过到跳表的不同前置元素。

这个名称来自于大量使用 [CTZ 指令]，它让我们能够高效地计算 2 的幂因子。对于给定的块 *n*，该块包含 ctz(*n*) + 1 个指针。

跳表的附加指针让我们能够比单链表更有效地在磁盘上导航数据结构。

考虑从数据块 5 到数据块 1 的路径。你可以看到数据块 3 被完全跳过了：

我们可以通过查看到达任何块的路径的运行时复杂性来找到它。路径上的每一步将搜索空间减半，使我们的运行时复杂性为 *O(log n)*。要到达包含最多指针的块，我们可以反向执行相同的步骤，这将运行时复杂性为 *O(2 log n)* = *O(log n)*。一个有趣的说明是，这种最优路径在我们贪心地选择覆盖最大距离而不超过目标的指针时自然产生。

因此，我们现在有一个 COW 数据结构，附加成本低且易于附加，运行时最坏情况为 *O(n log n)*。鉴于这个运行时也被我们可以存储在块中的数据量所分割，这个成本相当合理。

------

这是一种新的数据结构，因此我们仍有几个问题。存储开销是多少？指针数量能否超过一个块的大小？我们如何在元数据对中存储 CTZ 跳表？

为了找到存储开销，我们可以将数据结构看作多个链表。每个链表跳过的块数是前一个链表的两倍，或者从另一个角度看，每个链表使用的存储量是前一个链表的一半。当我们趋近于无穷大时，存储开销形成一个几何级数。解决这个问题告诉我们，平均我们的存储开销仅为每块 2 个指针。

因为我们的文件大小有限，存储大小的字长，我们也可以求出在一个块中需要存储的指针的最大数量。如果我们将指针的开销设为块大小，我们得到以下方程。注意，更小的块大小（B）和更大的字长（w）都会导致更多的存储开销。

通过对方程 B 求解，我们得到一些常见字长的最小块大小：

1. 32 位 CTZ 跳表 => 最小块大小 104 字节
2. 64 位 CTZ 跳表 => 最小块大小 448 字节

littlefs 使用 32 位字长，因此只有块小于 104 字节时，指针才会溢出。这是一个简单的要求，因为在实践中，大多数块大小从 512 字节开始。只要块大小大于 104 字节，我们就可以避免处理指针溢出所需的额外逻辑。

最后一个问题是如何在我们的元数据对中存储 CTZ 跳表？我们需要指向头块的指针，跳表的大小，头块的索引，以及头块的偏移量。但值得注意的是，每个大小映射到一个唯一的索引 + 偏移量对。因此理论上我们只需要存储一个指针和大小。

然而，从大小计算索引 + 偏移量对有点复杂。我们可以开始用一个求和公式，循环遍历直到给定的大小。设 B 是块大小（字节），w 是字长（位），n 是跳表中块的索引，N 是文件大小（字节）：

N = ∑(i=0到n) [B - (w/8)(ctz(i) + 1)]

这个公式效果很好，但需要 *O(n)* 来计算，这将文件读取的完整运行时复杂度提升到 *O(n² log n)*。幸运的是，这个求和不需要访问磁盘，因此实际上影响最小。

然而，尽管整合了位操作，我们实际上可以将这个方程减少为 *O(1)* 形式。在浏览令人惊叹的资源 [整数序列在线百科全书 (OEIS)] 时，我发现 [A001511] 匹配 CTZ 指令的迭代，和 [A005187] 匹配其部分求和。令人惊讶的是，这些都来自简单的方程，导致我们发现了一个相当直观的性质，将两个看似无关的位操作联系起来：

∑(i=0到n) (ctz(i) + 1) = 2n - popcount(n)

其中：

1. ctz(x) = x 中尾随 0 的位数
2. popcount(x) = x 中 1 的位数

最初的测试表明这个令人惊讶的性质似乎成立。随着 n 趋近于无穷大，我们最终得到平均开销为 2 个指针，这与我们之前的假设相符。在迭代过程中，popcount 函数似乎处理了这种平均值的偏差。当然，为了确保这一点，我写了一个快速脚本验证了这个性质对于所有 32 位整数都成立。

现在我们可以将其代入原始方程，找到更高效的文件大小方程：

N = Bn - (w/8)(2n - popcount(n))

不幸的是，popcount 函数是非单射的，因此我们无法解这个方程得到我们的索引。但我们可以解一个 n' 索引，它大于 n，并且误差被 popcount 函数的范围所限制。我们可以反复将 n' 代入原始方程，直到误差小于我们的整数分辨率。事实证明，我们只需要执行一次代入，这给出了我们的索引公式：

n = floor((N - (w/8) popcount(N / (B - 2w/8))) / (B - 2w/8))

现在我们有了索引 n，我们只需将其代入上述方程即可找到偏移量。我们在整数溢出时遇到了一些问题，但我们可以通过稍微重新排列方程来避免：

off = N - (B - 2w/8)n - (w/8) popcount(n)

我们的解决方案需要相当多的数学，但计算机在数学方面非常擅长。现在我们可以在 *O(1)* 时间内从大小找到块索引和偏移量，让我们只需一个指针和大小即可存储 CTZ 跳表。

CTZ 跳表为我们提供了一个 COW 数据结构，附加成本低且易于附加，运行时最坏情况为 *O(n log n)*。鉴于这个运行时也被我们可以存储在块中的数据量所分割，这个成本相当合理。结合元数据对，CTZ 跳表提供了抗断电性和紧凑的数据存储。

## 块分配器

现在我们有了一个原子、磨损均衡的文件系统框架。小型双块元数据对提供了原子更新，而 CTZ 跳表则提供了 COW 块中数据的紧凑存储。

但现在我们需要看房间里的“象”：所有这些块来自哪里？

决定下一个要使用的块是块分配器的责任。在文件系统设计中，块分配通常是次要的，但在 COW 文件系统中，它的角色变得更加重要，因为几乎每次写入都需要它。

通常，块分配涉及某种形式的空闲列表或位图存储在文件系统上，并且需要更新这些空闲块。然而，对于抗断电性，保持这些结构一致变得困难。任何在更新这些结构中的错误都会导致丢失的块，无法恢复。

littlefs 采取了一种谨慎的方法。littlefs 不信任磁盘上的空闲列表，而是依赖于文件系统在磁盘上是自由块的镜像这一事实。块分配器的操作类似于脚本语言中的垃圾收集器，根据需要扫描以查找未使用的块。

尽管这种方法听起来复杂，但不维护空闲列表大大简化了 littlefs 的整体设计。与编程语言不同，只有少数几种数据结构需要遍历。块释放，几乎与块分配一样频繁，简单地是一个无操作。这种“随意丢弃”策略大大减少了管理磁盘上数据结构的复杂性，特别是在处理高风险错误条件时。

------

我们的块分配器需要高效地找到空闲块。你可以遍历存储上的每个块，并将每个块与我们的文件系统树进行检查；然而，运行时会非常糟糕。我们需要以某种方式每次遍历收集多个块。

看现有设计，一些使用类似“随意丢弃”策略的较大文件系统在 RAM 中存储整个存储的位图。这很好，因为位图非常紧凑。但我们不能在这里使用相同的策略，因为它违反了我们的恒定 RAM 要求，但我们可能能够将这个想法修改成可行的解决方案。

littlefs 中的块分配器是介于磁盘大小位图和蛮力遍历之间的折衷方案。我们不使用整个存储大小的位图，而是保持一个小的、固定大小的位图，称为前瞻缓冲区。在块分配期间，我们从前瞻缓冲区中取出块。如果前瞻缓冲区为空，我们扫描文件系统以寻找更多空闲块，填充我们的前瞻缓冲区。在每次扫描中，我们使用递增的偏移量，随着块的分配绕行存储。

这种前瞻方法的运行时复杂性为完全扫描存储是 *O(n²)*；然而，位图非常紧凑，并且在实践中通常只需要一两次遍历即可找到空闲块。此外，通过调整块大小或前瞻缓冲区的大小，可以优化分配器的性能，在写入粒度或 RAM 与分配器性能之间进行权衡。

## 磨损均衡

块分配器有一个次要角色：磨损均衡。

磨损均衡是将磨损分布在存储的所有块上，以防止文件系统由于存储中单个块的磨损而过早失效的过程。

littlefs 有两种保护措施来防止磨损：

1. 检测并从坏块中恢复
2. 将磨损均匀分布在动态块上

------

从坏块恢复实际上与块分配器本身无关。而是依赖于文件系统在坏块发生时检测和驱逐坏块的能力。

在 littlefs 中，检测坏块在写入时相当直接。所有写入都必须由某种形式的 RAM 中的数据源，因此在写入块后，我们可以立即读取数据并验证其是否正确写入。如果发现磁盘上的数据与我们在 RAM 中的副本不匹配，则写入错误已经发生，我们很可能有一个坏块。

一旦检测到坏块，我们需要从中恢复。在写入错误的情况下，我们在 RAM 中有损坏数据的副本，因此我们只需要驱逐坏块，分配一个新的（希望是好的）块，并重复先前失败的写入。

驱逐坏块并用新块替换它的实际行为留给文件系统的复制有界写入（CObW）数据结构。CObW 数据结构的一个属性是任何块都可以在 COW 操作期间替换。有界写入部分通常由计数器触发，但没有任何东西阻止我们在发现坏块后立即触发 COW 操作。

```
Diagram removed
```

我们可能会发现新块也是坏的，但希望通过重复这个循环，我们最终会找到一个新的块，写入成功。如果没有，这意味着存储中的所有块都是坏的，我们已经达到了设备可用寿命的尽头。这时，littlefs 将返回一个“空间不足”错误。这在技术上是正确的，因为没有更多好的块了，但作为附加好处，它也符合动态大小数据用户期望的错误条件。

------

另一方面，读取错误则复杂得多。我们在 RAM 中没有数据的副本，因此我们需要一种方式在数据被损坏后仍能重建原始数据。其中一种机制是错误更正码（ECC）。

ECC 是校验和概念的扩展。像 CRC 这样的校验和可以检测数据中是否发生了错误，而 ECC 可以检测并实际修正一些错误。然而，ECC 能检测的错误数量是有限的：汉明界限。随着错误数量接近汉明界限，我们仍然可以检测错误，但无法修复数据。如果达到这个点，块是无法恢复的。

littlefs 本身不提供 ECC。块的性质和 ECC 的相对大占用不适合文件系统的动态大小数据，修正错误而无需 RAM 是复杂的，且 ECC 更适合块设备的几何形态。事实上，几个 NOR 闪存芯片有用于 ECC 的额外存储，并且许多 NAND 芯片甚至可以在芯片本身计算 ECC。

在 littlefs 中，ECC 完全是可选的。读取错误可以通过磨损均衡主动预防。但重要的是要注意，ECC 可以在块设备级别使用，以适度延长设备的寿命。littlefs 尊重块设备报告的任何错误，允许块设备提供更积极的错误检测。

------

为了避免读取错误，我们需要采取主动措施，而不是像处理写入错误那样采取被动措施。

一种方法是检测块中的错误数量是否超过某个阈值，但仍然可恢复。有了 ECC，我们可以在写入时做到这一点，并将错误视为写入错误，在致命读取错误有机会发展之前驱逐块。

另一种，更通用的策略，是主动将磨损均匀分布在存储的所有块上，希望没有单个块在存储的其余部分接近其可用寿命终点之前失败。这被称为磨损均衡。

通常，磨损均衡算法分为两类：

1. 动态磨损均衡，其中我们将磨损分布在“动态”块上。这可以通过仅考虑未使用的块来实现。
2. 静态磨损均衡，其中我们将磨损分布在“动态”和“静态”块上。为使其工作，我们需要考虑所有块，包括已经包含数据的块。

为了代码大小和复杂性的权衡，littlefs（目前）仅提供动态磨损均衡。这是一个尽力而为的解决方案。磨损并没有被完美地分布，但它在空闲块之间分布，极大地延长了设备的寿命。

此外，littlefs 使用统计磨损均衡算法。这意味着我们不主动跟踪磨损，而是依赖存储中磨损的均匀分布来近似动态磨损均衡算法。尽管这个名称很长，但实际上它是动态磨损均衡的一种简化。

磨损的均匀分布留给块分配器，它在两部分中创建了均匀分布。容易的一部分是在设备通电时分配块时，我们按线性顺序分配块，循环存储。更困难的一部分是设备失去电源时该怎么办。我们不能只是从存储开始重新启动分配器，因为这会偏向磨损。相反，我们每次挂载文件系统时都以随机偏移量启动分配器。只要这个随机偏移量是均匀的，分配模式的组合也是均匀分布的。

尽管这种磨损均衡方法看起来需要依赖于独立于电源的随机数生成器，每次启动都必须返回不同的随机数，这似乎是一个困难的依赖。然而，文件系统处于一个相对独特的情况，因为它位于一个跨越断电持久存在的大量熵之上。

实际上，我们可以直接使用磁盘上的数据来驱动我们的随机数生成器。在实践中，这是通过对每个元数据对的校验和进行异或来实现的，这在已经计算用于获取和挂载文件系统时完成。

请注意，这个随机数生成器并不完美。它只在文件系统被修改时返回唯一的随机数。这正是我们希望在分配器中分布磨损的，因此可以近似动态磨损均衡算法。但这意味着这个随机数生成器对通用用途来说并不有用。

------

结合坏块检测和动态磨损均衡，我们提供了一种尽力而为的解决方案，以避免文件系统因磨损而过早失效。重要的是，littlefs 的磨损均衡算法提供了一个关键特性：你可以通过增加存储容量来延长设备的寿命。而如果需要更积极的磨损均衡，你可以始终将 littlefs 与闪存转换层（FTL）结合使用，获得一个带有静态磨损均衡的小型抗断电文件系统。

## 文件

现在我们已经完成了构建模块，开始全面审视我们的文件系统。

第一步：我们如何实际存储文件？

我们已经确定 CTZ 跳表在紧凑地存储数据方面相当不错，因此遵循其他文件系统的先例，我们可以为每个文件提供一个存储在元数据对中的跳表，作为文件的 inode。

然而，这在文件很小时效果不好，这在嵌入式系统中很常见。与 PC 相比，嵌入式系统的所有数据都是小的。

考虑一个小的 4 字节文件。使用两个块的元数据对和一个块的 CTZ 跳表，我们发现自己使用了完整的 3 块。在大多数具有 4 KiB 块的 NOR 闪存中，这是 12 KiB 的开销。3072 倍的增加，简直荒谬。

我们可以做出几项改进。首先，不是为每个文件分配自己的元数据对，而是将多个文件存储在一个元数据对中。实现这一点的一种方法是将目录直接与一个元数据对（或一系列元数据对）关联。这让多个文件可以共享目录的元数据对以进行日志记录，并减少集体存储开销。

元数据对和目录的严格绑定还让用户可以根据他们组织目录的方式直接控制存储利用率。

第二个改进是，我们注意到对于非常小的文件，我们试图使用 CTZ 跳表进行紧凑存储适得其反。元数据对有大约 4 倍的存储开销，因此如果我们的文件小于块大小的 1/4，将文件存储在元数据对之外实际上没有好处。

在这种情况下，我们可以直接在目录的元数据对中存储文件。我们称之为内联文件，它允许目录高效地存储许多小文件。我们之前的 4 字节文件现在只占用理论上的 16 字节磁盘空间。

一旦文件超过 1/4 块大小，我们切换到 CTZ 跳表。这意味着我们的文件永远不会使用超过 4 倍的存储开销，随着文件大小的增长而减少。

## 目录

现在我们只需要目录来存储我们的文件。正如上面提到的，我们希望目录和元数据对严格绑定，但有一些需要解决的复杂性。

单独来看，每个目录是一个元数据对的链表。这让我们能够在每个目录中存储无限数量的文件，我们不需要担心无限日志的运行时复杂性。我们可以在元数据对中存储其他目录的指针，这为我们提供了一个目录树，就像其他文件系统中的一样。

主要的复杂性是，一旦我们希望使用恒定 RAM 遍历目录树。目录树是一个树，不幸的是，事实是你不能用恒定 RAM 遍历一棵树。

幸运的是，我们的树的元素是元数据对，因此与 CTZ 跳表不同，我们不受限于严格的 COW 操作。我们可以做的是在线程一个链表穿过我们的树，明确地启用对整个文件系统的廉价遍历。

不坚持纯 COW 操作带来了一些问题。现在，每当我们想操作目录树时，需要更新多个指针。如果你熟悉设计原子数据结构，这应该会引起一堆红色警报。

为了解决这个问题，我们的线程链表有一些余地。它不仅包含文件系统中存在的元数据对，还允许包含因断电而没有父目录的元数据对。这些称为孤立的元数据对。

有了孤立的可能性，我们可以构建保持文件系统树和线程链表的断电抗性操作。

添加目录到我们的树：

移除目录：

除了正常的目录树操作，我们还可以使用孤立来驱逐元数据对中的块，当块坏掉或超过其分配的擦除次数时。如果在驱逐元数据块时断电，我们可能会出现文件系统引用替换块，而线程链表仍然包含被驱逐块的情况。我们称之为半孤立。

找到孤立和半孤立是昂贵的，需要 *O(n²)* 次比较每个元数据对与每个目录条目。但权衡是一个断电抗性文件系统，且只使用有限的 RAM。幸运的是，我们只需要在启动后的第一次分配时检查孤立，而只读的 littlefs 可以完全忽略线程链表。

如果我们只有某种全局状态，我们也可以存储一个标志，并避免在不知道具体被中断时搜索孤立（预言！）。

## 移动问题

我们有最后一个挑战：移动问题。简单地说，问题是：

如何原子地将文件从一个目录移动到另一个目录？

在 littlefs 中，我们可以原子地提交到目录，但无法创建跨多个目录的原子提交。文件系统必须经过至少两个不同的状态才能完成移动。

更糟的是，文件移动是文件系统同步的常见形式。作为一个设计用于断电的文件系统，正确处理原子移动非常重要。

那么我们能做什么？

- 我们绝对不能让断电导致文件被复制或丢失。这可能会轻易地破坏用户的代码，并且只会在极端情况下暴露。我们之所以能够对线程链表采取懒惰态，是因为它不是面向用户的，我们可以在内部处理角落情况。
- 一些文件系统将 COW 操作向上传播，直到找到共同的父目录。不幸的是，这与我们的线程树交互不佳，并且重新引入了磨损向上传播的问题。
- 在 littlefs 的早期版本中，我们尝试通过在源和目标之间来回切换，标记和取消标记文件正在移动，从而使移动在用户视角下是原子的。这有效了，但效果不佳。找到失败的移动是昂贵的，并且需要每个文件的唯一标识符。

最终，解决移动问题需要创建一种新的机制，以便在多个元数据对之间共享知识。在 littlefs 中，这导致引入了一种称为“全局状态”的机制。

------

全局状态是一小组可以从 *任何* 元数据对更新的状态。将全局状态与元数据对在一个提交中更新多个条目的能力结合起来，为我们提供了一个强大的工具，用于制作复杂的原子操作。

全局状态是如何工作的？

全局状态存在为分布在文件系统中的元数据对的一组增量。通过将文件系统中所有增量进行异或，全局状态实际上可以通过这些增量构建出来。

要从元数据对更新全局状态，我们将我们已知的全局状态与我们的更改和元数据对中任何现有的增量一起异或。将这个新的增量提交到元数据对即提交了文件系统的全局状态更改。

为了使其高效，我们总是在 RAM 中保留全局状态的副本。只有在挂载文件系统时，我们才需要迭代元数据对并构建全局状态。

你可能已经注意到，全局状态非常昂贵。我们在 RAM 中保留一个副本，并在无限数量的元数据对中存储一个增量。即使我们将全局状态重置为其初始值，我们也无法轻松清理磁盘上的增量。因此，保持全局状态有界且极小非常重要。但即使有严格的预算，全局状态也极具价值。

------

现在我们可以解决移动问题。我们可以在创建新文件时创建描述移动的全局状态，并在移除旧文件时清除此移动状态，从而使移动从用户视角上是原子的。

如果在挂载期间构建全局状态时，我们发现描述一个正在进行的移动的信息，我们知道在移动期间失去电源，文件在源目录和目标目录中被复制。如果发生这种情况，我们可以使用全局状态中的信息来移除其中一个文件，从而解决移动。

我们也可以将目录同样移动。存在线程链表，但保持线程链表不变就可以了，因为顺序并不重要。

## 结论

这就是 littlefs，感谢阅读！